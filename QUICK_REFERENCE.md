# Referencia RÃ¡pida - Llama Workshop

> GuÃ­a de referencia rÃ¡pida para comandos y conceptos comunes

## ğŸš€ Comandos Esenciales

### Ollama

```bash
# GestiÃ³n de modelos
ollama list                    # Listar modelos instalados
ollama pull llama3.2:3b       # Descargar modelo
ollama rm llama3.2:3b         # Eliminar modelo
ollama show llama3.2:3b       # Info del modelo

# Ejecutar modelos
ollama run llama3.2:3b        # Modo interactivo
ollama run llama3.2:3b "prompt"  # Prompt directo

# Servidor
ollama serve                  # Iniciar servidor
```

### Python - Ollama

```python
import ollama

# Chat bÃ¡sico
response = ollama.chat(
    model='llama3.2:3b',
    messages=[{'role': 'user', 'content': 'Hola'}]
)

# GeneraciÃ³n simple
response = ollama.generate(
    model='llama3.2:3b',
    prompt='Escribe un haiku'
)

# Streaming
for chunk in ollama.chat(model='llama3.2:3b', messages=msgs, stream=True):
    print(chunk['message']['content'], end='')
```

### Git

```bash
# Actualizar repositorio
git pull origin main

# Ver estado
git status

# Crear rama
git checkout -b mi-feature

# Commit y push
git add .
git commit -m "DescripciÃ³n"
git push origin mi-feature
```

## ğŸ“Š ParÃ¡metros Comunes

### Temperature
```
0.1-0.3  â†’ Respuestas precisas, deterministas
0.4-0.7  â†’ Balanceado (default)
0.8-1.2  â†’ Creativo, variado
1.3-2.0  â†’ Muy creativo, impredecible
```

### Top-P (Nucleus Sampling)
```
0.9-0.95 â†’ Recomendado (default)
0.8      â†’ MÃ¡s conservador
0.99     â†’ MÃ¡s diverso
```

### Top-K
```
40-50    â†’ Recomendado
20-30    â†’ MÃ¡s conservador
60-80    â†’ MÃ¡s diverso
```

## ğŸ—‚ï¸ Estructura de Prompts

### Formato BÃ¡sico
```
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

[Tu sistema prompt aquÃ­]<|eot_id|><|start_header_id|>user<|end_header_id|>

[Tu pregunta aquÃ­]<|eot_id|><|start_header_id|>assistant<|end_header_id|>
```

### Template Efectivo
```
Rol: Eres un [rol especÃ­fico]
Contexto: [informaciÃ³n relevante]
Tarea: [quÃ© debe hacer]
Formato: [cÃ³mo debe responder]
Restricciones: [limitaciones]

[Input del usuario]
```

## ğŸ“š NavegaciÃ³n RÃ¡pida

### DocumentaciÃ³n
- [Inicio](README.md)
- [Getting Started](GETTING_STARTED.md)
- [Docs principales](docs/README.md)
- [Glosario](docs/glosario.md)
- [Anexos](docs/anexos.md)

### CÃ³digo
- [Ollama ejemplos](code/ollama/)
- [RAG ejemplos](code/rag/)

### Templates
- [Business Canvas](templates/canvas/)
- [Plan 30-60-90](templates/30-60-90/)
- [Prompts](templates/prompts/)

### Comunidad
- [Contribuir](CONTRIBUTING.md)
- [CÃ³digo de Conducta](CODE_OF_CONDUCT.md)
- [Issues](https://github.com/majorjuanjo/llama/issues)
- [Discussions](https://github.com/majorjuanjo/llama/discussions)

## ğŸ”§ Troubleshooting RÃ¡pido

| Error | SoluciÃ³n RÃ¡pida |
|-------|----------------|
| "ollama not found" | Reinicia terminal o agrega a PATH |
| "model not found" | `ollama pull nombre-modelo` |
| "out of memory" | Usa modelo mÃ¡s pequeÃ±o (1b o 3b) |
| "connection refused" | `ollama serve` en otra terminal |
| "module not found" | `pip install -r requirements.txt` |

## ğŸ’¡ Tips RÃ¡pidos

### Mejorar Respuestas
1. SÃ© especÃ­fico en tus prompts
2. Proporciona ejemplos (few-shot)
3. Ajusta temperature segÃºn necesidad
4. Usa sistema de prompts para contexto

### Optimizar Performance
1. Usa cuantizaciÃ³n apropiada (Q4, Q5, Q8)
2. Limita context window si no necesitas 128k
3. Batch requests cuando sea posible
4. Cachea respuestas comunes

### Mejores PrÃ¡cticas
1. Siempre maneja errores
2. Valida inputs del usuario
3. Limita longitud de respuestas
4. Monitorea uso de recursos
5. Documenta tus prompts

## ğŸ“Š TamaÃ±os de Modelo

| Modelo | ParÃ¡metros | RAM | Uso |
|--------|-----------|-----|-----|
| llama3.2:1b | 1B | 2GB | MÃ³vil, Edge |
| llama3.2:3b | 3B | 4GB | Chat general |
| llama3.1:8b | 8B | 8GB | PropÃ³sito general |
| llama3.1:70b | 70B | 48GB | Tareas complejas |

## ğŸ¯ Casos de Uso Comunes

### Chatbot
```python
from code.ollama import simple_chat
# Ver: code/ollama/simple_chat.py
```

### RAG
```python
from code.rag import simple_rag
# Ver: code/rag/simple_rag.py
```

### API
```python
from fastapi import FastAPI
# Ver: docs/04-capitulo-3.md
```

## ğŸ“ Soporte

- ğŸ“– [DocumentaciÃ³n completa](docs/)
- ğŸ› [Reportar bug](https://github.com/majorjuanjo/llama/issues/new?template=bug_report.md)
- ğŸ’¡ [Solicitar feature](https://github.com/majorjuanjo/llama/issues/new?template=feature_request.md)
- â“ [Hacer pregunta](https://github.com/majorjuanjo/llama/issues/new?template=question.md)

## ğŸ”— Enlaces Ãštiles

- [Ollama Docs](https://github.com/ollama/ollama)
- [Meta Llama](https://ai.meta.com/llama/)
- [LangChain](https://python.langchain.com/)
- [ChromaDB](https://docs.trychroma.com/)

---

**ğŸ’¾ Guarda esta pÃ¡gina como favorito para referencia rÃ¡pida**

[ğŸ  Volver al README](README.md)
