# 3.3. Implementaci√≥n RAG

## Implementaci√≥n de RAG: Conectando tus datos

Vamos a crear tu "Cerebro Corporativo". Tienes dos rutas para lograrlo.

#### üõ°Ô∏è Ruta A: Arquitectura Personalizada (C√≥digo Python)

* **Herramientas:** Python, LangChain, ChromaDB.
* **Perfil:** Ingenieros de datos o desarrolladores que requieren integraci√≥n profunda.
* **Ventaja:** Control total sobre c√≥mo se corta y busca la informaci√≥n.

#### ‚ö° Ruta B: Drop & Chat (Visual - AnythingLLM)

* **Herramientas:** AnythingLLM Desktop.
* **Perfil:** Due√±os de negocio, Analistas, Operaciones.
* **Ventaja:** Pasas de "cero" a "chat con documentos" en 15 minutos. Soberan√≠a local total.

***

#### Configuraci√≥n con AnythingLLM Desktop

Esta herramienta crea una base de datos vectorial local en tu m√°quina.

**Paso 1: Instalaci√≥n** Descarga e instala [useanything.com](https://useanything.com/download).

**Paso 2: Configuraci√≥n del Motor**

1. Abre la app. En "LLM Preference", selecciona **Ollama** (si lo instalaste en la sesi√≥n pasada) o **Groq** (si prefieres nube).
2. En "Vector Database", deja el default (**LanceDB**). Es local y r√°pido.

**Paso 3: Creaci√≥n del Workspace**

1. Crea un nuevo espacio llamado "Manuales de Empresa".
2. Arrastra tus PDFs al √°rea de carga.
3. Haz clic en **"Move to Workspace"** y luego **"Save and Embed"**.
   * _Aqu√≠ es donde ocurre la magia: tus textos se convierten en n√∫meros._

**Paso 4: La Prueba** Escribe en el chat: _"¬øCu√°l es la pol√≠tica de vi√°ticos seg√∫n el manual?"_ El sistema responder√° citando el documento.



\### Arquitectura RAG con LangChain

Para quienes desean integrar esto en su propio software.

**Requisitos:** `pip install langchain chromadb ollama`

```python
from langchain_community.llms import Ollama
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings

# 1. Cargar
loader = PyPDFLoader("manual_operaciones.pdf")
docs = loader.load_and_split()

# 2. Embeddings & VectorStore
embeddings = OllamaEmbeddings(model="llama3.2")
db = Chroma.from_documents(docs, embeddings)

# 3. Retrieval
query = "¬øCu√°l es el proceso de devoluci√≥n?"
docs = db.similarity_search(query)
print(docs[0].page_content)
```

\{% endtab %\} \{% endtabs %\}

\{% hint style="success" %\} VALIDACI√ìN DE √âXITO Haz una pregunta cuya respuesta est√© en la p√°gina 20 de un PDF. Si la IA te responde correctamente, tu RAG est√° vivo. \{% endhint %\}

```


```
