# Cap√≠tulo 1: Fundamentos de Llama

## üìñ Contenido

1. [Arquitectura T√©cnica](#arquitectura-t√©cnica)
2. [Formato de Prompts](#formato-de-prompts)
3. [Par√°metros de Generaci√≥n](#par√°metros-de-generaci√≥n)
4. [Cuantizaci√≥n](#cuantizaci√≥n)
5. [Optimizaci√≥n de Rendimiento](#optimizaci√≥n-de-rendimiento)

---

## Arquitectura T√©cnica

### Transformer Architecture

Llama se basa en la arquitectura Transformer con las siguientes optimizaciones:

- **RMSNorm**: Normalizaci√≥n mejorada para entrenamiento estable
- **SwiGLU**: Funci√≥n de activaci√≥n optimizada
- **RoPE**: Positional embeddings rotacionales
- **Grouped Query Attention**: Reducci√≥n de memoria en inferencia

### Especificaciones T√©cnicas

```
Llama 3.1 8B:
- Par√°metros: 8 mil millones
- Capas: 32
- Dimensi√≥n oculta: 4096
- Cabezas de atenci√≥n: 32
- Vocabulario: 128,256 tokens
- Longitud de contexto: 128k tokens
```

## Formato de Prompts

### Plantilla de Sistema

Llama utiliza tokens especiales para estructurar conversaciones:

```
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

Eres un asistente √∫til.<|eot_id|><|start_header_id|>user<|end_header_id|>

¬øCu√°l es la capital de M√©xico?<|eot_id|><|start_header_id|>assistant<|end_header_id|>
```

### Mejores Pr√°cticas para Prompts

1. **S√© Espec√≠fico**
   ```
   ‚ùå Malo: "Escribe sobre M√©xico"
   ‚úÖ Bueno: "Escribe un p√°rrafo de 100 palabras sobre la historia prehisp√°nica de M√©xico"
   ```

2. **Proporciona Contexto**
   ```
   ‚úÖ "Como experto en finanzas mexicanas, explica qu√© es el ISR..."
   ```

3. **Usa Ejemplos (Few-Shot)**
   ```
   Clasifica el sentimiento:
   
   Texto: "Me encant√≥ el producto"
   Sentimiento: Positivo
   
   Texto: "No funcion√≥ como esperaba"
   Sentimiento: Negativo
   
   Texto: "El servicio es excelente"
   Sentimiento: [Tu respuesta aqu√≠]
   ```

## Par√°metros de Generaci√≥n

### Par√°metros Principales

| Par√°metro | Rango | Descripci√≥n | Uso Recomendado |
|-----------|-------|-------------|-----------------|
| `temperature` | 0.0-2.0 | Creatividad vs Precisi√≥n | 0.1-0.3 (preciso), 0.7-1.0 (creativo) |
| `top_p` | 0.0-1.0 | Nucleus sampling | 0.9-0.95 |
| `top_k` | 1-100 | Limitaci√≥n de vocabulario | 40-50 |
| `max_tokens` | 1-128000 | Longitud m√°xima | Seg√∫n necesidad |
| `repeat_penalty` | 1.0-2.0 | Evita repeticiones | 1.1-1.2 |

### Ejemplos de Configuraci√≥n

```python
# Para respuestas precisas (documentaci√≥n, c√≥digo)
{
    "temperature": 0.1,
    "top_p": 0.95,
    "top_k": 40,
    "max_tokens": 2000
}

# Para contenido creativo (historias, marketing)
{
    "temperature": 0.8,
    "top_p": 0.95,
    "top_k": 50,
    "max_tokens": 1000
}

# Para conversaci√≥n balanceada
{
    "temperature": 0.7,
    "top_p": 0.9,
    "top_k": 40,
    "max_tokens": 500
}
```

## Cuantizaci√≥n

La cuantizaci√≥n reduce el tama√±o del modelo manteniendo calidad aceptable.

### Tipos de Cuantizaci√≥n

| Formato | Precisi√≥n | Tama√±o vs FP16 | Calidad | Velocidad |
|---------|-----------|----------------|---------|-----------|
| FP16 | 16-bit | 100% | Excelente | Lenta |
| Q8 | 8-bit | ~50% | Muy buena | Media |
| Q5 | 5-bit | ~31% | Buena | R√°pida |
| Q4 | 4-bit | ~25% | Aceptable | Muy r√°pida |
| Q2 | 2-bit | ~12.5% | Limitada | Extremadamente r√°pida |

### Recomendaciones de Cuantizaci√≥n

```
Llama 3.1 8B:
- Producci√≥n: Q8 o Q5_K_M
- Desarrollo: Q4_K_M
- Edge/M√≥vil: Q4_0 o Q2_K

Llama 3.1 70B:
- Producci√≥n: Q5_K_M
- Desarrollo: Q4_K_M
```

## Optimizaci√≥n de Rendimiento

### 1. Hardware

**CPU**:
- AMD Ryzen 7/9 o Intel i7/i9
- M√≠nimo 8 n√∫cleos recomendado

**GPU** (opcional pero recomendado):
- NVIDIA: RTX 3060+ (12GB VRAM m√≠nimo)
- AMD: RX 6800+ 
- Apple: M1 Pro+ (Metal acceleration)

**RAM**:
- 8B model: 16GB m√≠nimo, 32GB recomendado
- 70B model: 64GB+ m√≠nimo

### 2. Configuraci√≥n de Sistema

```bash
# Linux: Aumentar l√≠mites de memoria
ulimit -l unlimited

# Optimizar swap
sudo sysctl vm.swappiness=10

# Para modelos grandes (70B+)
sudo sysctl vm.overcommit_memory=1
```

### 3. Batch Processing

```python
# Procesar m√∫ltiples prompts eficientemente
prompts = [
    "Pregunta 1...",
    "Pregunta 2...",
    "Pregunta 3..."
]

# Procesamiento en lotes reduce overhead
responses = model.generate(prompts, batch_size=3)
```

### 4. Context Management

```python
# Limita el contexto para mejor rendimiento
max_context = 4096  # En lugar de 128k para respuestas r√°pidas

# Usa sliding window para conversaciones largas
if len(conversation) > max_context:
    conversation = conversation[-max_context:]
```

## üéØ Ejercicio Pr√°ctico

1. **Experimenta con Temperature**:
   - Genera 5 respuestas con temperature=0.1
   - Genera 5 respuestas con temperature=1.5
   - Compara la variabilidad

2. **Cuantizaci√≥n**:
   - Calcula cu√°nta RAM necesitas para Llama 8B en Q4 vs Q8
   - ¬øQu√© formato es adecuado para tu hardware?

3. **Dise√±o de Prompt**:
   - Dise√±a un prompt efectivo para tu caso de uso
   - Incluye contexto, ejemplos y formato esperado

---

## üìö Recursos T√©cnicos

- [Llama Model Card](https://github.com/meta-llama/llama-models)
- [GGUF Format Documentation](https://github.com/ggerganov/ggml)
- [Optimization Techniques](https://huggingface.co/docs/transformers/llm_tutorial_optimization)

## ‚û°Ô∏è Siguiente Paso

Avanza al [Cap√≠tulo 2: Configuraci√≥n del Entorno](./03-capitulo-2.md) para comenzar con la instalaci√≥n pr√°ctica.
