# Anexos

## üìñ Contenido

1. [Comandos √ötiles de Ollama](#comandos-√∫tiles-de-ollama)
2. [Comparativa de Modelos](#comparativa-de-modelos)
3. [Recursos de Aprendizaje](#recursos-de-aprendizaje)
4. [Comunidades y Foros](#comunidades-y-foros)
5. [Datasets P√∫blicos](#datasets-p√∫blicos)
6. [Troubleshooting Avanzado](#troubleshooting-avanzado)

---

## Comandos √ötiles de Ollama

### Gesti√≥n de Modelos

```bash
# Listar modelos instalados
ollama list

# Mostrar informaci√≥n de un modelo
ollama show llama3.1:8b

# Ver modelo con m√°s detalles
ollama show llama3.1:8b --modelfile

# Eliminar modelo
ollama rm llama3.1:8b

# Copiar modelo
ollama cp llama3.1:8b mi-modelo-custom
```

### Ejecuci√≥n y Testing

```bash
# Ejecutar modelo interactivamente
ollama run llama3.1:8b

# Ejecutar con prompt directo
ollama run llama3.1:8b "Explica qu√© es la IA"

# Modo multilinea
ollama run llama3.1:8b """
Este es un prompt
de m√∫ltiples l√≠neas
"""

# Con par√°metros personalizados
ollama run llama3.1:8b \
  --temperature 0.9 \
  --top-k 50 \
  "Escribe un poema"
```

### API y Desarrollo

```bash
# Iniciar servidor
ollama serve

# Especificar puerto
OLLAMA_HOST=0.0.0.0:11434 ollama serve

# Ver logs
journalctl -u ollama -f  # Linux con systemd

# Reiniciar servicio
systemctl restart ollama  # Linux
```

### Importaci√≥n y Exportaci√≥n

```bash
# Crear modelo desde Modelfile
ollama create mi-modelo -f ./Modelfile

# Exportar modelo
ollama push usuario/modelo

# Importar modelo
ollama pull usuario/modelo
```

## Comparativa de Modelos

### Modelos Llama

| Modelo | Par√°metros | RAM M√≠n. | Velocidad | Calidad | Uso Recomendado |
|--------|-----------|----------|-----------|---------|-----------------|
| Llama 3.2 1B | 1B | 2GB | ‚ö°‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | M√≥vil, Edge |
| Llama 3.2 3B | 3B | 4GB | ‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | Chat ligero |
| Llama 3.1 8B | 8B | 8GB | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Prop√≥sito general |
| Llama 3.1 70B | 70B | 48GB | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Tareas complejas |
| Llama 3.1 405B | 405B | 256GB | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Estado del arte |

### Otros Modelos en Ollama

| Modelo | Especialidad | Tama√±o | Idioma |
|--------|-------------|--------|--------|
| Mistral | Prop√≥sito general | 7B | Multiling√ºe |
| Mixtral | Mixture of Experts | 8x7B | Multiling√ºe |
| CodeLlama | Programaci√≥n | 7B-34B | C√≥digo |
| Phi-3 | Eficiente | 3.8B | Ingl√©s |
| Gemma | Google | 2B-7B | Multiling√ºe |

### Comparativa de Cuantizaci√≥n

```
Modelo: Llama 3.1 8B

FP16 (Full Precision):
- Tama√±o: ~16GB
- RAM necesaria: ~18GB
- Calidad: 100%
- Velocidad: Base

Q8_0:
- Tama√±o: ~8.5GB
- RAM necesaria: ~10GB
- Calidad: ~99%
- Velocidad: +20%

Q5_K_M:
- Tama√±o: ~5.3GB
- RAM necesaria: ~7GB
- Calidad: ~97%
- Velocidad: +40%

Q4_K_M:
- Tama√±o: ~4.7GB
- RAM necesaria: ~6GB
- Calidad: ~95%
- Velocidad: +60%

Q2_K:
- Tama√±o: ~3.5GB
- RAM necesaria: ~5GB
- Calidad: ~85%
- Velocidad: +100%
```

## Recursos de Aprendizaje

### Documentaci√≥n Oficial

- [Meta Llama](https://ai.meta.com/llama/)
- [Ollama Docs](https://github.com/ollama/ollama)
- [LangChain Docs](https://python.langchain.com/)
- [Hugging Face](https://huggingface.co/docs)

### Cursos Recomendados

**Gratuitos:**
- [DeepLearning.AI - Generative AI](https://www.deeplearning.ai/short-courses/)
- [Fast.ai - Practical Deep Learning](https://course.fast.ai/)
- [Google - Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course)

**De Pago:**
- [Coursera - Natural Language Processing Specialization](https://www.coursera.org/specializations/natural-language-processing)
- [Udacity - AI Product Manager](https://www.udacity.com/course/ai-product-manager-nanodegree--nd088)

### Libros

**T√©cnicos:**
- "Attention Is All You Need" - Paper original de Transformers
- "Deep Learning" - Ian Goodfellow
- "Natural Language Processing with Transformers" - Lewis Tunstall

**Aplicados:**
- "Building LLM Apps" - Valentina Alto
- "Generative AI with LangChain" - Ben Auffarth
- "AI Engineering" - Chip Huyen

### Blogs y Newsletters

- [The Batch](https://www.deeplearning.ai/the-batch/) - DeepLearning.AI
- [Import AI](https://jack-clark.net/) - Jack Clark
- [The Gradient](https://thegradient.pub/)
- [Hugging Face Blog](https://huggingface.co/blog)

## Comunidades y Foros

### Internacionales

- [r/LocalLLaMA](https://reddit.com/r/LocalLLaMA) - Reddit
- [Ollama Discord](https://discord.gg/ollama)
- [Hugging Face Forums](https://discuss.huggingface.co/)
- [LangChain Discord](https://discord.gg/langchain)

### En Espa√±ol

- [r/es_inteligencia_artificial](https://reddit.com/r/es_inteligencia_artificial)
- [IA en Espa√±ol - Telegram](https://t.me/IAenEspanol)
- [Python M√©xico](https://www.facebook.com/groups/pythonmexico/)

### Eventos

- **Meta AI Developer Events**
- **PyData Conferences**
- **ICLR, NeurIPS, ICML** - Conferencias acad√©micas
- **Meetups locales de IA/ML**

## Datasets P√∫blicos

### Para Fine-Tuning en Espa√±ol

```python
# Datasets de Hugging Face
datasets = [
    "bertin-project/alpaca-spanish",
    "somosnlp/somos-clean-alpaca-es",
    "mrm8488/alpaca-es",
    "clibrain/Spanish-WikiQA",
]

# Cargar con datasets library
from datasets import load_dataset

dataset = load_dataset("bertin-project/alpaca-spanish")
```

### Datos Generales

| Dataset | Tama√±o | Idioma | Uso |
|---------|--------|--------|-----|
| Common Crawl | Petabytes | Multi | Preentrenamiento |
| Wikipedia | ~20GB | Multi | Conocimiento general |
| OpenWebText | ~40GB | EN | Preentrenamiento |
| C4 | ~750GB | Multi | Preentrenamiento |
| The Pile | ~800GB | EN | Investigaci√≥n |

### Datasets Especializados

**C√≥digo:**
- The Stack
- CodeSearchNet
- GitHub Code

**Conversaci√≥n:**
- ShareGPT
- OpenAssistant
- Anthropic HH-RLHF

**Espa√±ol:**
- BETO datasets
- SQuAD-es
- XNLI-es

## Troubleshooting Avanzado

### Error: "Out of Memory"

```bash
# Soluci√≥n 1: Usar modelo m√°s peque√±o
ollama pull llama3.2:1b

# Soluci√≥n 2: Aumentar swap (Linux)
sudo dd if=/dev/zero of=/swapfile bs=1G count=8
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile

# Soluci√≥n 3: Usar cuantizaci√≥n m√°s agresiva
ollama pull llama3.1:8b-q4_0
```

### Error: "Model not found"

```bash
# Verificar modelos instalados
ollama list

# Re-descargar modelo
ollama pull llama3.1:8b

# Verificar permisos (Linux/Mac)
ls -la ~/.ollama/models/

# Limpiar cach√© corrupto
rm -rf ~/.ollama/models/manifests/*
```

### Problema: Respuestas Lentas

```python
# Optimizaci√≥n 1: Reducir contexto
ollama.generate(
    model='llama3.1:8b',
    prompt=prompt,
    options={'num_ctx': 2048}  # En lugar de 128k
)

# Optimizaci√≥n 2: Usar modelo cuantizado
# Q4 es ~2x m√°s r√°pido que Q8

# Optimizaci√≥n 3: Batch processing
# Procesar m√∫ltiples prompts juntos

# Optimizaci√≥n 4: GPU acceleration
# Verificar que Ollama use GPU
nvidia-smi  # NVIDIA
rocm-smi    # AMD
```

### Error: "Connection Refused"

```bash
# Verificar que Ollama est√© corriendo
ps aux | grep ollama

# Iniciar Ollama
ollama serve

# Verificar puerto
lsof -i :11434  # Mac/Linux
netstat -ano | findstr :11434  # Windows

# Cambiar puerto
OLLAMA_HOST=0.0.0.0:11435 ollama serve
```

### Problema: Calidad de Respuestas

```python
# Ajustar temperature
# M√°s bajo = m√°s predecible
# M√°s alto = m√°s creativo
options = {
    'temperature': 0.1,  # Para respuestas precisas
    'top_p': 0.9,
    'top_k': 40
}

# Mejorar prompt
prompt = """Contexto: [proporciona contexto relevante]

Tarea: [describe claramente lo que quieres]

Formato esperado: [especifica el formato]

Restricciones: [a√±ade limitaciones si es necesario]
"""

# Usar sistema de prompts
messages = [
    {
        'role': 'system',
        'content': 'Eres un experto en [dominio]. Responde de manera [estilo].'
    },
    {
        'role': 'user',
        'content': 'Tu pregunta aqu√≠'
    }
]
```

## üìä Benchmarks de Rendimiento

### Tokens por Segundo (promedio)

```
Hardware: RTX 3080 (10GB)
Modelo: Llama 3.1 8B Q4

Batch Size 1:  ~45 tokens/s
Batch Size 4:  ~120 tokens/s
Batch Size 8:  ~180 tokens/s

Hardware: M1 Pro (16GB)
Modelo: Llama 3.1 8B Q4

Batch Size 1:  ~28 tokens/s
Batch Size 4:  ~75 tokens/s

Hardware: CPU (i7-12700K, 32GB RAM)
Modelo: Llama 3.1 8B Q4

Batch Size 1:  ~12 tokens/s
Batch Size 4:  ~30 tokens/s
```

## üîß Scripts √ötiles

### Verificaci√≥n Completa del Sistema

```bash
#!/bin/bash
# check_system.sh

echo "=== Verificaci√≥n del Sistema para Llama ==="
echo

echo "1. Ollama instalado:"
ollama --version

echo "2. Modelos disponibles:"
ollama list

echo "3. Python instalado:"
python3 --version

echo "4. Paquetes Python:"
pip list | grep -E "ollama|langchain|chromadb"

echo "5. Memoria disponible:"
free -h  # Linux
# vm_stat  # macOS

echo "6. Espacio en disco:"
df -h ~/.ollama

echo "7. GPU disponible:"
nvidia-smi 2>/dev/null || echo "No NVIDIA GPU"

echo
echo "=== Verificaci√≥n completa ==="
```

### Benchmark de Modelos

```python
# benchmark.py
import time
import ollama
import statistics

def benchmark_model(model, prompts, num_runs=3):
    """Benchmark de un modelo"""
    times = []
    
    for _ in range(num_runs):
        for prompt in prompts:
            start = time.time()
            ollama.generate(model=model, prompt=prompt)
            elapsed = time.time() - start
            times.append(elapsed)
    
    return {
        'model': model,
        'avg_time': statistics.mean(times),
        'min_time': min(times),
        'max_time': max(times),
        'std_dev': statistics.stdev(times) if len(times) > 1 else 0
    }

# Uso
prompts = [
    "¬øQu√© es Python?",
    "Explica la teor√≠a de la relatividad",
    "Dame una receta de tacos"
]

models = ['llama3.2:1b', 'llama3.2:3b', 'llama3.1:8b']

for model in models:
    try:
        result = benchmark_model(model, prompts)
        print(f"\nModelo: {result['model']}")
        print(f"Tiempo promedio: {result['avg_time']:.2f}s")
        print(f"Desv. est√°ndar: {result['std_dev']:.2f}s")
    except Exception as e:
        print(f"Error con {model}: {e}")
```

---

## üìö Referencias

- [Ollama GitHub](https://github.com/ollama/ollama)
- [Meta Llama 3.1](https://ai.meta.com/blog/meta-llama-3-1/)
- [LangChain Documentation](https://python.langchain.com/)
- [Hugging Face Hub](https://huggingface.co/)

## ‚û°Ô∏è Siguiente

Consulta el [Glosario](./glosario.md) para t√©rminos t√©cnicos.
