# Glosario de T√©rminos

## üìö Terminolog√≠a de IA y LLMs

### A

**Agent (Agente)**
Sistema de IA que puede tomar decisiones y realizar acciones de manera aut√≥noma, utilizando herramientas y planificaci√≥n para alcanzar objetivos.

**API (Application Programming Interface)**
Interfaz que permite la comunicaci√≥n entre diferentes aplicaciones de software. En el contexto de Llama, se refiere a c√≥mo interactuar con el modelo program√°ticamente.

**Attention Mechanism (Mecanismo de Atenci√≥n)**
Componente clave de los Transformers que permite al modelo enfocarse en partes relevantes del input al generar output.

### B

**Batch Processing (Procesamiento por Lotes)**
T√©cnica de procesar m√∫ltiples entradas simult√°neamente para mejorar eficiencia.

**BPTT (Back-Propagation Through Time)**
Algoritmo de entrenamiento para redes neuronales recurrentes.

### C

**Checkpoint**
Estado guardado de un modelo durante el entrenamiento, permite reanudar o usar el modelo desde ese punto.

**ChromaDB**
Base de datos vectorial especializada en almacenar y buscar embeddings para sistemas RAG.

**Context Length (Longitud de Contexto)**
Cantidad m√°xima de tokens que un modelo puede procesar simult√°neamente. Llama 3.1 soporta hasta 128k tokens.

**Context Window (Ventana de Contexto)**
Porci√≥n del texto que el modelo puede "ver" al mismo tiempo para generar respuestas.

**Cuantizaci√≥n (Quantization)**
Proceso de reducir la precisi√≥n num√©rica de los pesos del modelo para disminuir tama√±o y aumentar velocidad.
- **Q8**: 8 bits - alta calidad
- **Q4**: 4 bits - equilibrio
- **Q2**: 2 bits - m√°xima compresi√≥n

### D

**Decoder-Only Architecture**
Arquitectura de Transformer que solo usa el componente decoder. Llama utiliza esta arquitectura.

**Deployment (Despliegue)**
Proceso de poner un modelo de IA en producci√≥n para uso real.

**Docker**
Plataforma de contenedorizaci√≥n que permite empaquetar aplicaciones con sus dependencias.

### E

**Embedding**
Representaci√≥n vectorial de texto que captura significado sem√°ntico. Palabras similares tienen embeddings similares.

**Epoch**
Una pasada completa por todo el dataset de entrenamiento.

**Evaluation (Evaluaci√≥n)**
Proceso de medir el rendimiento de un modelo usando m√©tricas espec√≠ficas.

### F

**FastAPI**
Framework moderno de Python para crear APIs web de alto rendimiento.

**Few-Shot Learning**
T√©cnica donde se proporcionan pocos ejemplos al modelo para guiar sus respuestas.

**Fine-Tuning**
Proceso de entrenar un modelo preentrenado con datos espec√≠ficos para adaptarlo a un caso de uso particular.

**FP16 (16-bit Floating Point)**
Formato num√©rico de 16 bits usado en deep learning, balance entre precisi√≥n y memoria.

### G

**GGUF (GPT-Generated Unified Format)**
Formato de archivo para modelos de lenguaje optimizado para inferencia eficiente.

**GPU (Graphics Processing Unit)**
Procesador especializado en c√°lculos paralelos, ideal para ejecutar modelos de IA.

**Gradient**
Derivada que indica la direcci√≥n para actualizar los pesos del modelo durante entrenamiento.

### H

**Hallucination (Alucinaci√≥n)**
Cuando un LLM genera informaci√≥n incorrecta o inventada que parece plausible.

**Hugging Face**
Plataforma y comunidad para compartir modelos, datasets y herramientas de IA.

**Hyperparameter (Hiperpar√°metro)**
Configuraci√≥n externa al modelo que afecta el entrenamiento o inferencia (ej: temperature, learning rate).

### I

**Inference (Inferencia)**
Proceso de usar un modelo entrenado para hacer predicciones o generar outputs.

**Instruction Tuning**
Fine-tuning espec√≠fico para que el modelo siga instrucciones mejor.

### K

**Knowledge Base (Base de Conocimiento)**
Colecci√≥n de informaci√≥n estructurada o no estructurada usada por sistemas RAG.

### L

**LangChain**
Framework para desarrollar aplicaciones con LLMs, facilita cadenas de procesamiento y agentes.

**Latency (Latencia)**
Tiempo que tarda el modelo en responder a una solicitud.

**LLM (Large Language Model)**
Modelo de lenguaje grande entrenado en vastas cantidades de texto. Ejemplo: Llama 3.1.

**LoRA (Low-Rank Adaptation)**
T√©cnica eficiente de fine-tuning que modifica solo una peque√±a parte del modelo.

### M

**Memory (Memoria)**
En contexto de chatbots, el historial de conversaci√≥n que se mantiene para coherencia.

**Mixtral**
Modelo tipo Mixture of Experts con m√∫ltiples sub-modelos especializados.

**Model Card**
Documento que describe un modelo de IA: arquitectura, capacidades, limitaciones y uso √©tico.

**Modelfile**
Archivo de configuraci√≥n de Ollama que define par√°metros y comportamiento de un modelo.

### N

**Natural Language Processing (NLP)**
Campo de IA enfocado en la interacci√≥n entre computadoras y lenguaje humano.

**Neural Network (Red Neuronal)**
Sistema de aprendizaje autom√°tico inspirado en el cerebro humano.

**Normalization (Normalizaci√≥n)**
T√©cnica para estabilizar y acelerar el entrenamiento de redes neuronales.

### O

**Ollama**
Herramienta para ejecutar LLMs localmente de forma sencilla.

**One-Shot Learning**
T√©cnica donde se proporciona un solo ejemplo al modelo.

**Overfitting**
Cuando un modelo aprende demasiado de los datos de entrenamiento y pierde capacidad de generalizaci√≥n.

### P

**Parameter (Par√°metro)**
Valor aprendido durante el entrenamiento del modelo. Llama 3.1 8B tiene 8 mil millones de par√°metros.

**Perplexity**
M√©trica que mide qu√© tan "sorprendido" est√° el modelo con el texto. Menor perplexity = mejor.

**Prompt**
Texto de entrada que se proporciona a un LLM para generar una respuesta.

**Prompt Engineering**
Arte y ciencia de dise√±ar prompts efectivos para obtener mejores resultados.

### Q

**Quantization** ‚Üí Ver Cuantizaci√≥n

**Query**
Pregunta o b√∫squeda realizada a un sistema, especialmente en contexto de RAG.

### R

**RAG (Retrieval-Augmented Generation)**
T√©cnica que combina b√∫squeda de informaci√≥n con generaci√≥n de texto para respuestas m√°s precisas.

**RLHF (Reinforcement Learning from Human Feedback)**
T√©cnica de entrenamiento usando feedback humano para alinear el modelo con preferencias humanas.

**RoPE (Rotary Position Embedding)**
M√©todo de encoding posicional usado en Llama para manejar secuencias largas.

### S

**Sampling**
Proceso de seleccionar el siguiente token durante la generaci√≥n de texto.

**Semantic Search (B√∫squeda Sem√°ntica)**
B√∫squeda basada en significado, no solo palabras clave.

**Streaming**
Env√≠o de respuestas del modelo token por token en tiempo real.

**System Prompt**
Instrucci√≥n inicial que define el comportamiento y personalidad del asistente.

### T

**Temperature**
Par√°metro que controla aleatoriedad en la generaci√≥n:
- Baja (0.1-0.3): Respuestas m√°s deterministas
- Alta (0.8-1.5): Respuestas m√°s creativas

**Token**
Unidad b√°sica de texto procesada por el modelo (palabra, subpalabra o car√°cter).

**Tokenization (Tokenizaci√≥n)**
Proceso de convertir texto en tokens.

**Top-K Sampling**
T√©cnica de sampling que considera solo los K tokens m√°s probables.

**Top-P (Nucleus Sampling)**
T√©cnica de sampling que considera tokens hasta alcanzar probabilidad acumulada P.

**Transfer Learning**
Uso de conocimiento de un modelo preentrenado para una tarea relacionada.

**Transformer**
Arquitectura de red neuronal basada en atenci√≥n, base de modelos como Llama.

### U

**Underfitting**
Cuando un modelo es demasiado simple y no captura patrones en los datos.

**Upstream/Downstream Tasks**
Upstream: Tareas de preentrenamiento. Downstream: Tareas espec√≠ficas posteriores.

### V

**Vector Database (Base de Datos Vectorial)**
Base de datos optimizada para almacenar y buscar embeddings. Ejemplo: ChromaDB.

**Vector Embedding** ‚Üí Ver Embedding

**Vision-Language Model (VLM)**
Modelo que procesa tanto im√°genes como texto.

### W

**Weight (Peso)**
Valor num√©rico en la red neuronal que se ajusta durante el entrenamiento.

**Workflow**
Secuencia de pasos en un proceso de IA/ML.

### Z

**Zero-Shot Learning**
Capacidad del modelo de realizar tareas sin ejemplos previos de esa tarea espec√≠fica.

---

## üìä Acr√≥nimos Comunes

| Acr√≥nimo | Significado | Descripci√≥n |
|----------|-------------|-------------|
| AI | Artificial Intelligence | Inteligencia Artificial |
| API | Application Programming Interface | Interfaz de programaci√≥n |
| BERT | Bidirectional Encoder Representations from Transformers | Modelo de lenguaje bidireccional |
| CLI | Command Line Interface | Interfaz de l√≠nea de comandos |
| CPU | Central Processing Unit | Procesador central |
| DL | Deep Learning | Aprendizaje profundo |
| GPU | Graphics Processing Unit | Unidad de procesamiento gr√°fico |
| GUI | Graphical User Interface | Interfaz gr√°fica de usuario |
| LLM | Large Language Model | Modelo de lenguaje grande |
| ML | Machine Learning | Aprendizaje autom√°tico |
| NLP | Natural Language Processing | Procesamiento de lenguaje natural |
| QA | Question Answering | Sistema de preguntas y respuestas |
| RAG | Retrieval-Augmented Generation | Generaci√≥n aumentada por recuperaci√≥n |
| RLHF | Reinforcement Learning from Human Feedback | Aprendizaje por refuerzo con feedback humano |
| SFT | Supervised Fine-Tuning | Fine-tuning supervisado |
| VRAM | Video RAM | Memoria de video (GPU) |

---

## üî¢ Unidades y Medidas

**Par√°metros:**
- 1B = 1 mil millones (billion) de par√°metros
- 1M = 1 mill√≥n de par√°metros
- 1K = 1 mil par√°metros

**Memoria:**
- 1 TB = 1024 GB
- 1 GB = 1024 MB
- 1 MB = 1024 KB

**Tokens:**
- ~1 token ‚âà 0.75 palabras en ingl√©s
- ~1 token ‚âà 0.5-1 palabra en espa√±ol
- 1000 tokens ‚âà 750 palabras

**Contexto:**
- 4K tokens ‚âà 3 p√°ginas de texto
- 8K tokens ‚âà 6 p√°ginas de texto
- 128K tokens ‚âà 96 p√°ginas de texto

---

## üìñ Recursos Adicionales

Para t√©rminos m√°s espec√≠ficos o t√©cnicos, consulta:

- [Papers With Code Glossary](https://paperswithcode.com/)
- [Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/)
- [Google ML Glossary](https://developers.google.com/machine-learning/glossary)

---

## ‚¨ÖÔ∏è Volver

Regresa a [README principal](./README.md) o revisa los [Anexos](./anexos.md).
